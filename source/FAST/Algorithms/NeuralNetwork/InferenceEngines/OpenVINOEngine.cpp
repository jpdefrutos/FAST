#include "OpenVINOEngine.hpp"
#include <inference_engine.hpp>
#include <FAST/Utility.hpp>
#include <ngraph/ngraph.hpp>

namespace fast {

using namespace InferenceEngine;

void OpenVINOEngine::run() {
    std::lock_guard<std::mutex> lock(m_mutex);
	try {
		// Copy input data
		reportInfo() << "OpenVINO: Processing input nodes.." << reportEnd();
		int batchSize = -1;
		for(const auto& node : mInputNodes) {
			auto tensor = node.second.data;
			batchSize = tensor->getShape()[0];
			auto access = tensor->getAccess(ACCESS_READ);
			float* tensorData = access->getRawData();
			Blob::Ptr input = m_inferRequest->GetBlob(node.first);

			// Dynamic batch size
			if(m_maxBatchSize > 1)
                m_inferRequest->SetBatch(batchSize);

			auto input_data = input->buffer().as<PrecisionTrait<Precision::FP32>::value_type * >();
			std::memcpy(input_data, tensorData, input->byteSize());
		}
		reportInfo() << "OpenVINO: Finished processing input nodes." << reportEnd();

		// Execute network
        m_inferRequest->Infer();
		reportInfo() << "OpenVINO: Network executed." << reportEnd();

		// Copy output data
		for (auto& node : mOutputNodes) {
			Blob::Ptr output = m_inferRequest->GetBlob(node.first);
			auto outputData = (output->buffer().as<::InferenceEngine::PrecisionTrait<Precision::FP32>::value_type *>());
			auto copied_data = make_uninitialized_unique<float[]>(output->byteSize());
			std::memcpy(copied_data.get(), outputData, output->byteSize());
			auto tensor = Tensor::create(std::move(copied_data), node.second.shape);
			node.second.data = tensor;
		}
		reportInfo() << "OpenVINO: Finished processing output nodes." << reportEnd();
	}
	catch (::InferenceEngine::details::InferenceEngineException &e) {
		throw Exception("Inference error occured during OpenVINO::run: " + std::string(e.what()));
	}
}

void OpenVINOEngine::loadPlugin(std::string deviceName) {

    reportInfo() << "OpenVINO: Inference plugin setup for device type " << deviceName << reportEnd();

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    auto input_model = getFilename();
    CNNNetwork network;
    if(input_model.empty()) { // If filename is not set, load from memory instead
        // Read from memory
        std::string strModel(m_model.begin(), m_model.end());
        network = m_inferenceCore->ReadNetwork(strModel, make_shared_blob<uint8_t>({Precision::U8, {m_weights.size()}, C}, m_weights.data()));
    } else {
        // Read from file
        if (!fileExists(input_model))
            throw FileNotFoundException(input_model);
        network = m_inferenceCore->ReadNetwork(input_model);
    }

    // Check if model has undefined input shape using ngraph: https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_ShapeInference.html
    if(network.getFunction()) { // Only if ngraph function is available (onnx)
        bool needsReshape = false;
        std::map<std::string, SizeVector> inputSizes;
        const auto parameters = network.getFunction()->get_parameters();
        for (const auto &parameter : parameters) {
            auto name = parameter->get_friendly_name();
            auto partialShape = parameter->get_partial_shape();
            SizeVector shape;
            for (int i = 0; i < partialShape.rank().get_length(); ++i) {
                if (partialShape[i].is_dynamic()) {
                    needsReshape = true;
                    if (i == 0) { // If first dimension is unknown, set it to 1 (batch size)
                        shape.push_back(1);
                    } else {
                        // TODO get input size from user
                        throw Exception("Input shape of neural network node " + name + " was not fully specified.");
                    }
                } else {
                    shape.push_back(partialShape[i].get_length());
                }
            }
            inputSizes[name] = shape;
        }
        if(needsReshape)
            network.reshape(inputSizes);
    }

    //network.setBatchSize(1);
    reportInfo() << "OpenVINO: Network loaded." << reportEnd();

    // --------------------------- Prepare input blobs -----------------------------------------------------
    int inputCount = 0;
    int outputCount = 0;
    const bool inputsDefined = !mInputNodes.empty();
    const bool outputsDefined = !mOutputNodes.empty();
    for(auto& input : network.getInputsInfo()) {
        auto input_info = input.second;
        auto name = input.first;
        input_info->setPrecision(Precision::FP32);
        // TODO shape is reverse direction here for some reason..
        TensorShape shape;
        auto data = input_info->getInputData();
        auto dims = data->getDims();
        for(int i = 0; i < dims.size(); ++i) { // TODO why reverse??
            shape.addDimension(dims[i]);
        }

        const auto layout = input_info->getLayout();
        NodeType type;
        if(layout == Layout::NCHW || layout == Layout::NCDHW || layout == Layout::NHWC || layout == Layout::NDHWC) {
            // TODO There is a bug in OpenVINO here: layout detection does not work properly for channel last/first
            /*
            if(layout == Layout::NCHW || layout == Layout::NCDHW) {
                m_imageOrdering = ImageOrdering::ChannelFirst;
            } else {
                m_imageOrdering = ImageOrdering::ChannelLast;
            }*/
            // Try to determine channel ordering automatically due to bug above
            if(shape[shape.getDimensions() - 1] <= 4) {
                m_imageOrdering = ImageOrdering::ChannelLast;
                reportInfo() << "Guessed image ordering to be channel last as shape was " << shape.toString()
                             << reportEnd();
            } else {
                m_imageOrdering = ImageOrdering::ChannelFirst;
                reportInfo() << "Guessed image ordering to be channel first as shape was " << shape.toString()
                             << reportEnd();
            }
            type = NodeType::IMAGE;
        } else {
            type = NodeType::TENSOR;
        }
        if(inputsDefined) {
            if(mInputNodes.count(name) > 0) {
                reportInfo() << "Node was defined by user at id " << mInputNodes[name].portID  << reportEnd();
                if(mInputNodes[name].shape.empty())
                    mInputNodes[name].shape = shape;
            } else {
                reportInfo() << "Ignored input node " << name << " because input nodes were specified, but not this one." << reportEnd();
            }
        } else {
            addInputNode(inputCount, name, type, shape);
            ++inputCount;
        }
        reportInfo() << "Found input node: " << name << " with shape " << shape.toString() << reportEnd();
    }

    // --------------------------- Prepare output blobs ----------------------------------------------------
    for(auto& output : network.getOutputsInfo()) {
        auto info = output.second;
        auto name = output.first;
        info->setPrecision(Precision::FP32);
        TensorShape shape;
        for(auto dim : info->getDims())
            shape.addDimension(dim);
        reportInfo() << "Found output node " << name << " with shape " << shape.toString() << reportEnd();
        if(outputsDefined) {
            if(mOutputNodes.count(name) > 0) {
                reportInfo() << "Node was defined by user at id " << mOutputNodes[name].portID  << reportEnd();
                if(mOutputNodes[name].shape.empty()) {
                    reportInfo() << "Shape was empty, setting it to " << shape.toString() << reportEnd();
                    mOutputNodes[name].shape = shape;
                }
            } else {
                reportInfo() << "Ignored output node " << name << " because output nodes were specified, but not this one." << reportEnd();
            }
        } else {
            addOutputNode(outputCount, name, NodeType::TENSOR, shape);
            ++outputCount;
        }
    }
    reportInfo() << "OpenVINO: Node setup complete." << reportEnd();

    std::map<std::string, std::string> config;
    if(m_maxBatchSize > 1) {
        config[PluginConfigParams::KEY_DYN_BATCH_ENABLED] = PluginConfigParams::YES;
        network.setBatchSize(m_maxBatchSize);
    }

    ExecutableNetwork executable_network = m_inferenceCore->LoadNetwork(network, deviceName, config);

    m_inferRequest = executable_network.CreateInferRequestPtr();
    setIsLoaded(true);
    reportInfo() << "OpenVINO: Network fully loaded." << reportEnd();
}

void OpenVINOEngine::load() {
    if(!m_inferenceCore)
        m_inferenceCore = std::make_shared<Core>();
    auto devices = m_inferenceCore->GetAvailableDevices();
    reportInfo() << "Available OpenVINO devices:" << reportEnd();
    for(auto&& device : devices)
        reportInfo() << device << reportEnd();
    if(m_deviceType == InferenceDeviceType::ANY) {
        try {
            loadPlugin("GPU");
        } catch(::InferenceEngine::details::InferenceEngineException &e) {
            try {
                reportInfo() << "Failed to get GPU plugin for OpenVINO inference engine: " << e.what() << reportEnd();
                reportInfo() << "Trying CPU plugin instead.." << reportEnd();
                loadPlugin("CPU");
            } catch(::InferenceEngine::details::InferenceEngineException &e) {
                reportError() << e.what() << reportEnd();
                throw Exception("Failed to load any device in OpenVINO IE");
            }
        }
    } else {
        std::map<InferenceDeviceType, std::string> deviceMapping = {
                {InferenceDeviceType::GPU, "GPU"},
                {InferenceDeviceType::CPU, "CPU"},
                {InferenceDeviceType::VPU, "MYRIAD"},
        };

        try {
            loadPlugin(deviceMapping[m_deviceType]);
        } catch(::InferenceEngine::details::InferenceEngineException &e) {
            throw Exception(std::string("Failed to load device ") + deviceMapping[m_deviceType] + " in OpenVINO inference engine");
        }
    }
}



std::vector<InferenceDeviceInfo> OpenVINOEngine::getDeviceList() {
    if(!m_inferenceCore)
        m_inferenceCore = std::make_shared<Core>();
    auto devices = m_inferenceCore->GetAvailableDevices();
    std::vector<InferenceDeviceInfo> result;
    for(auto&& device : devices) {
        InferenceDeviceInfo fastDeviceInfo;
        if(device.substr(0, 3) == "CPU") {
            fastDeviceInfo.type = InferenceDeviceType::CPU;
        } else if(device.substr(0,3) == "GPU"){
            fastDeviceInfo.type = InferenceDeviceType::GPU;
        } else if(device.substr(0,6) == "MYRIAD") {
            fastDeviceInfo.type = InferenceDeviceType::VPU;
        }
        fastDeviceInfo.index = 0;
        if(device.find('.') != std::string::npos) {
            fastDeviceInfo.index = std::stoi(device.substr(device.find('.')+1));
        }
        result.push_back(fastDeviceInfo);
    }

    return result;
}

ImageOrdering OpenVINOEngine::getPreferredImageOrdering() const {
    if(!isLoaded())
        throw Exception("Network must be loaded before calling getPreferredImageOrdering on TensorRTEngine");

    return m_imageOrdering;
}

std::string OpenVINOEngine::getName() const {
    return "OpenVINO";
}

OpenVINOEngine::~OpenVINOEngine() {
    //if(m_inferState != nullptr)
    //    delete m_inferState;
}

void OpenVINOEngine::loadCustomPlugins(std::vector<std::string> filenames) {
    if(isLoaded())
        throw Exception("Call loadCustomPlugins before load()");

    if(!m_inferenceCore)
        m_inferenceCore = std::make_shared<Core>();

    for(auto&& filename : filenames) {
        if(!fileExists(filename))
            throw FileNotFoundException(filename);
        if(stringToLower(filename.substr(filename.length()-4)) == "xml") {
            m_inferenceCore->SetConfig({ { ::InferenceEngine::PluginConfigParams::KEY_CONFIG_FILE, filename} }, "GPU"); // TODO how to detect VPU?
        } else {
            auto extension_ptr = make_so_pointer<::InferenceEngine::IExtension>(filename);
            m_inferenceCore->AddExtension(extension_ptr, "CPU");
        }
    }
}

}
